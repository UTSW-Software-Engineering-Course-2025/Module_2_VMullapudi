{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcfc1c6c",
   "metadata": {},
   "source": [
    "# Variational AutoEncoder (VAE)\n",
    "Train a VAE on the MNIST handwritten digit dataset. \n",
    "\n",
    "This makes use of: the custom Keras model class defined in vaegan.vae.py, our\n",
    "class for loading the MNIST dataset defined in vaegan.data, and our custom Keras\n",
    "callback in vaegan.callbacks.\n",
    "\n",
    "A directory called 'output' will be created to save figures and the trained\n",
    "model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpochs = 20  # orig, longer, more reasonable amount of training\n",
    "# nEpochs = 3  # quick testing during development\n",
    "%set_env CUDA_VISIBLE_DEVICES=1\n",
    "%cd 04_ExerciseCodeAndInstructions/vaegan\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bdb97eb",
   "metadata": {},
   "source": [
    "## 1. Import 3rd party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5dd15d5",
   "metadata": {},
   "source": [
    "## 2. Import our own classes (that we will complete together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5907ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our own classes\n",
    "from vaegan.data import MNIST\n",
    "from vaegan.callbacks import SaveImages\n",
    "import vaegan.vae\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "532ae5a7",
   "metadata": {},
   "source": [
    "## 3. Show some our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d269a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist yet.\n",
    "output_dir = \"./outputs/mnist_vae\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Instantiate the MNIST class containing our training data.\n",
    "data = MNIST()\n",
    "\n",
    "# Show some example images and their labels.\n",
    "data.show_example_images(os.path.join(output_dir, \"example_images.png\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66bfe090",
   "metadata": {},
   "source": [
    "## 4. Construct the model using the python class you completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyModule = reload(vaegan.vae)\n",
    "\n",
    "# Create the model. Note that we're using mostly the default arguments, but this is\n",
    "# where you might want to play around with different loss weights.\n",
    "tf.random.set_seed(1234)\n",
    "model = pyModule.VAE()\n",
    "\n",
    "# This step tells Keras to compute the explicit output shapes of each layer.\n",
    "# Otherwise, the layers will have dynamic/variable output shapes which is not\n",
    "# compatible with saving and loading.\n",
    "\n",
    "# TODO why isn't this working?\n",
    "model.compute_output_shape((None, 32, 32, 1))\n",
    "model.decoder.compute_output_shape((None, 8))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d73d6832",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Correct model dimensions </span>\n",
    "    \n",
    "\n",
    "<span style=\"color:blue\"> === OVERALL MODEL ==== </span>\n",
    "\n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "encoder (Encoder)            multiple                  240896    \n",
    "_________________________________________________________________\n",
    "decoder (Decoder)            multiple                  182145    \n",
    "_________________________________________________________________\n",
    "recon_loss (Mean)            multiple                  2         \n",
    "_________________________________________________________________\n",
    "kl_loss (Mean)               multiple                  2         \n",
    "_________________________________________________________________\n",
    "total_loss (Mean)            multiple                  2         \n",
    "=================================================================\n",
    "Total params: 423,047\n",
    "Trainable params: 422,337\n",
    "Non-trainable params: 710\n",
    "\n",
    "```\n",
    "<span style=\"color:blue\"> === ENCODER SUBMODEL ====</span>\n",
    "```\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_4 (InputLayer)            [(None, 32, 32, 1)]  0                                            \n",
    "__________________________________________________________________________________________________\n",
    "conv0 (Conv2D)                  (None, 32, 32, 16)   272         input_4[0][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "bn0 (BatchNormalization)        (None, 32, 32, 16)   64          conv0[1][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "relu0 (ReLU)                    (None, 32, 32, 16)   0           bn0[1][0]                        \n",
    "__________________________________________________________________________________________________\n",
    "conv1 (Conv2D)                  (None, 16, 16, 32)   8224        relu0[1][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "bn1 (BatchNormalization)        (None, 16, 16, 32)   128         conv1[1][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "relu1 (ReLU)                    (None, 16, 16, 32)   0           bn1[1][0]                        \n",
    "__________________________________________________________________________________________________\n",
    "conv2 (Conv2D)                  (None, 8, 8, 64)     32832       relu1[1][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "bn2 (BatchNormalization)        (None, 8, 8, 64)     256         conv2[1][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "relu2 (ReLU)                    (None, 8, 8, 64)     0           bn2[1][0]                        \n",
    "__________________________________________________________________________________________________\n",
    "conv3 (Conv2D)                  (None, 4, 4, 64)     65600       relu2[1][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "bn3 (BatchNormalization)        (None, 4, 4, 64)     256         conv3[1][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "relu3 (ReLU)                    (None, 4, 4, 64)     0           bn3[1][0]                        \n",
    "__________________________________________________________________________________________________\n",
    "flatten (Flatten)               (None, 1024)         0           relu3[1][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "dense (Dense)                   (None, 128)          131200      flatten[1][0]                    \n",
    "__________________________________________________________________________________________________\n",
    "dense_mean (Dense)              (None, 8)            1032        dense[1][0]                      \n",
    "__________________________________________________________________________________________________\n",
    "dense_logvar (Dense)            (None, 8)            1032        dense[1][0]                      \n",
    "==================================================================================================\n",
    "Total params: 240,896\n",
    "Trainable params: 240,544\n",
    "Non-trainable params: 352\n",
    "\n",
    "```\n",
    "<span style=\"color:blue\"> === DECODER SUBMODEL ====</span>\n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_5 (InputLayer)         [(None, 8)]               0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 1024)              9216      \n",
    "_________________________________________________________________\n",
    "relu_dense (ReLU)            (None, 1024)              0         \n",
    "_________________________________________________________________\n",
    "reshape (Reshape)            (None, 4, 4, 64)          0         \n",
    "_________________________________________________________________\n",
    "tconv0 (Conv2DTranspose)     (None, 8, 8, 64)          65600     \n",
    "_________________________________________________________________\n",
    "bn0 (BatchNormalization)     (None, 8, 8, 64)          256       \n",
    "_________________________________________________________________\n",
    "relu0 (ReLU)                 (None, 8, 8, 64)          0         \n",
    "_________________________________________________________________\n",
    "tconv1 (Conv2DTranspose)     (None, 16, 16, 64)        65600     \n",
    "_________________________________________________________________\n",
    "bn1 (BatchNormalization)     (None, 16, 16, 64)        256       \n",
    "_________________________________________________________________\n",
    "relu1 (ReLU)                 (None, 16, 16, 64)        0         \n",
    "_________________________________________________________________\n",
    "tconv2 (Conv2DTranspose)     (None, 32, 32, 32)        32800     \n",
    "_________________________________________________________________\n",
    "bn2 (BatchNormalization)     (None, 32, 32, 32)        128       \n",
    "_________________________________________________________________\n",
    "relu2 (ReLU)                 (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "tconv3 (Conv2DTranspose)     (None, 32, 32, 16)        8208      \n",
    "_________________________________________________________________\n",
    "bn3 (BatchNormalization)     (None, 32, 32, 16)        64        \n",
    "_________________________________________________________________\n",
    "relu3 (ReLU)                 (None, 32, 32, 16)        0         \n",
    "_________________________________________________________________\n",
    "conv_out (Conv2D)            (None, 32, 32, 1)         17        \n",
    "_________________________________________________________________\n",
    "sigmoid_out (Activation)     (None, 32, 32, 1)         0         \n",
    "=================================================================\n",
    "Total params: 182,145\n",
    "Trainable params: 181,793\n",
    "Non-trainable params: 352\n",
    "\n",
    "```\n",
    "## 5. Now check your model's  dimensions against this list above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2843eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"=== OVERALL MODEL ====\")\n",
    "model.summary()\n",
    "print(\"=== ENCODER SUBMODEL ====\")\n",
    "enc_in = tf.keras.layers.Input(model.image_shape)\n",
    "enc_out = model.encoder.call(enc_in)\n",
    "enc = tf.keras.Model(enc_in, enc_out)\n",
    "enc.summary()\n",
    "print(\"=== DECODER SUBMODEL ====\")\n",
    "dec_in = tf.keras.layers.Input((model.n_latent_dims,))\n",
    "dec_out = model.decoder.call(dec_in)\n",
    "dec = tf.keras.Model(dec_in, dec_out)\n",
    "dec.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f91e6f8",
   "metadata": {},
   "source": [
    "## 6. Compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e21403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with an optimizer. The learning rate of the optimizer can be\n",
    "# specified here. Normally, this is also where you would select a loss function\n",
    "# and any metrics. However, our custom model defines the loss functions inside\n",
    "# its __init__ constructor, so we don't need to do that here.\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "# Instantiate our custom callback to save a few example reconstructions and\n",
    "# generated images after each epoch.\n",
    "save_images_callback = SaveImages(\n",
    "    output_dir=output_dir,\n",
    "    model=model,\n",
    "    example_images=data.images_train[:10],\n",
    "    n_generated_images=10,\n",
    "    n_latent_dims=model.n_latent_dims,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6de88590",
   "metadata": {},
   "source": [
    "## 7. Train (fit) the model on the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64460794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model. Just like any off-the-shelf Keras model, we just call fit.\n",
    "# Under the hood, Keras will call the train_step method of our custom subclass\n",
    "# on each mini-batch and automatically loop through the training data. It will\n",
    "# take care of all the details, like converting numpy arrays to tensors, showing\n",
    "# a progress bar, and tracking the loss over the epochs.\n",
    "logs = model.fit(\n",
    "    data.images_train, batch_size=128, epochs=nEpochs, callbacks=[save_images_callback]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d419f689",
   "metadata": {},
   "source": [
    "## 8. Training saves results to disk, now also plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves, which are stored in logs.history as a dict. Keys of\n",
    "# this dict are the metric names, while the corresponding values are arrays.\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for loss_name in [\"recon_loss\", \"kl_loss\", \"total_loss\"]:\n",
    "    loss_values = logs.history[loss_name]\n",
    "    x = np.arange(len(loss_values))\n",
    "    ax.plot(x, loss_values, label=loss_name)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "fig.savefig(os.path.join(output_dir, \"training_curves.png\"), transparent=False)\n",
    "# fig.show()\n",
    "\n",
    "# Save the model\n",
    "model.save(os.path.join(output_dir, \"vae.keras\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
